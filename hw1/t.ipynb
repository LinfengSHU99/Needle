{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[[1,1,1],[1,2,3]], [[4,4,4],[7,7,7]]])\n",
    "a = np.array([[1,2,3,4], [2,3,4,5],[7,7,7,7]])\n",
    "print(a.sum())\n",
    "# b = a.sum(axis=0)\n",
    "# np.gradient(a)\n",
    "# a = torch.tensor([[1,2,3],[4,5,6]], requires_grad=True, dtype=torch.float32)\n",
    "# b = a.sum(dim=0)\n",
    "# c = b.sum()\n",
    "# c.backward()\n",
    "# c.grad\n",
    "# a.flat[5]\n",
    "a = np.random.randn(5,4)\n",
    "b = np.random.randn(5,4)\n",
    "a = np.array([1,2,3,4])\n",
    "# np.broadcast_to(a, (4,3))\n",
    "# c = a / a\n",
    "# c.shape\n",
    "# print(c)\n",
    "# a.shape\n",
    "# b = np.random.randn(1,2)\n",
    "# b @ a\n",
    "# t = torch.ones(4,3)\n",
    "# t2 = t + 1\n",
    "# a = torch.ones((2,3,4))\n",
    "# t = torch.unsqueeze(t, dim=0)\n",
    "# t2 = torch.unsqueeze(t2, dim=0)\n",
    "# b = torch.cat((t, t2), dim=0)\n",
    "# print(a)\n",
    "# print(t2)\n",
    "# a @ t2\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%a = np.array([1,2,3])\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%import needle as ndl\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([[-3.03661841],\n        [ 0.59958833],\n        [ 2.15711434],\n        [ 0.71221265],\n        [-0.77016609],\n        [ 3.72509045],\n        [-4.29146884],\n        [-2.43718602],\n        [ 0.05083794],\n        [-1.97262898]]),\n array([[-0.5819724 , -0.5819724 , -0.5819724 , -0.5819724 , -0.5819724 ,\n         -0.5819724 , -0.5819724 , -0.5819724 , -0.5819724 ],\n        [-0.13629693, -0.13629693, -0.13629693, -0.13629693, -0.13629693,\n         -0.13629693, -0.13629693, -0.13629693, -0.13629693],\n        [-0.62167499, -0.62167499, -0.62167499, -0.62167499, -0.62167499,\n         -0.62167499, -0.62167499, -0.62167499, -0.62167499],\n        [-0.27536644, -0.27536644, -0.27536644, -0.27536644, -0.27536644,\n         -0.27536644, -0.27536644, -0.27536644, -0.27536644],\n        [ 0.99912033,  0.99912033,  0.99912033,  0.99912033,  0.99912033,\n          0.99912033,  0.99912033,  0.99912033,  0.99912033],\n        [ 0.36154489,  0.36154489,  0.36154489,  0.36154489,  0.36154489,\n          0.36154489,  0.36154489,  0.36154489,  0.36154489],\n        [-0.6935182 , -0.6935182 , -0.6935182 , -0.6935182 , -0.6935182 ,\n         -0.6935182 , -0.6935182 , -0.6935182 , -0.6935182 ],\n        [ 1.04205859,  1.04205859,  1.04205859,  1.04205859,  1.04205859,\n          1.04205859,  1.04205859,  1.04205859,  1.04205859],\n        [ 0.50287113,  0.50287113,  0.50287113,  0.50287113,  0.50287113,\n          0.50287113,  0.50287113,  0.50287113,  0.50287113],\n        [-0.98266832, -0.98266832, -0.98266832, -0.98266832, -0.98266832,\n         -0.98266832, -0.98266832, -0.98266832, -0.98266832]])]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = [1,2,3]\n",
    "# a = list(map(lambda x: x-1, a))\n",
    "# print(a)\n",
    "\n",
    "# a = ndl.Tensor([1,2,3])\n",
    "# c = ndl.Tensor([1,2,3])\n",
    "# b = a * c\n",
    "# b.inputs\n",
    "from tests.test_autograd_hw import gradient_check\n",
    "# print(ndl.Tensor(np.random.randn(2,2,3)))\n",
    "# gradient_check(ndl.transpose, ndl.Tensor(np.random.randn(2,2,3)), axes=(2,0))\n",
    "# gradient_check(ndl.transpose, ndl.Tensor(np.random.randn(3, 5, 4)), axes=(1, 2))\n",
    "# gradient_check(ndl.transpose, ndl.Tensor(np.random.randn(3, 5, 4)), axes=(0, 1))\n",
    "# gradient_check(ndl.reshape, ndl.Tensor(np.random.randn(5, 4)), shape=(4, 5))\n",
    "# gradient_check(ndl.broadcast_to, ndl.Tensor(np.random.randn(3, 1)), shape=(3, 3))\n",
    "# gradient_check(ndl.broadcast_to, ndl.Tensor(np.random.randn(1,)), shape=(3, 3, 3))\n",
    "# gradient_check(ndl.broadcast_to, ndl.Tensor(np.random.randn(1, 3)), shape=(3, 3))\n",
    "# gradient_check(ndl.broadcast_to, ndl.Tensor(np.random.randn(5,4,1)), shape=(5,4,3))\n",
    "# gradient_check(ndl.broadcast_to, ndl.Tensor(np.random.randn(1)), shape=(3, 3, 3))\n",
    "# t1 = ndl.Tensor(np.random.randn(5, 4))\n",
    "# t2 = ndl.Tensor(np.random.randn(4, 5))\n",
    "# print(t1)\n",
    "# print(t2)\n",
    "# out = ndl.Tensor([[1, 1, 1, 1, 1,],\n",
    "#  [1, 1, 1, 1,1,],\n",
    "#  [1, 1, 1, 1, 1],\n",
    "#  [1 ,1, 1 ,1, 1],\n",
    "#  [1, 1, 1, 1 ,1]])\n",
    "# print(\"compute\")\n",
    "# print(t2 @ out)\n",
    "# print(out @ t1)\n",
    "# gradient_check(ndl.matmul, t1, t2)\n",
    "# gradient_check(ndl.matmul, ndl.Tensor(np.random.randn(5, 4)), ndl.Tensor(np.random.randn(4, 5)))\n",
    "# gradient_check(ndl.matmul, ndl.Tensor(np.random.randn(6, 6, 5, 4)), ndl.Tensor(np.random.randn(6, 6, 4, 3)))\n",
    "# gradient_check(ndl.matmul, ndl.Tensor(np.random.randn(2, 5, 4)), ndl.Tensor(np.random.randn(4, 3)))\n",
    "# gradient_check(ndl.matmul, ndl.Tensor(np.random.randn(5, 4)), ndl.Tensor(np.random.randn(6, 6, 4, 3)))\n",
    "#\n",
    "# gradient_check(ndl.negate, ndl.Tensor(np.random.randn(5, 4)))\n",
    "#\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(1,0))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4,1) + 1), axes=(0,1))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(0,))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(0,1))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4,1)), axes=(0,1))\n",
    "# gradient_check(ndl.divide_scalar, ndl.Tensor(np.random.randn(5, 4)), scalar=np.random.randn(1))\n",
    "# gradient_check(ndl.divide, ndl.Tensor(np.random.randn(5, 4)), ndl.Tensor(5 + np.random.randn(5, 4)))\n",
    "#\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(0,))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(0,))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4)), axes=(0,1))\n",
    "# gradient_check(ndl.summation, ndl.Tensor(np.random.randn(5,4,1)), axes=(0,1))\n",
    "# gradient_check(lambda A,B,C : ndl.summation((A@B+C)*(A@B), axes=None),\n",
    "#                    ndl.Tensor(np.random.randn(10,9)),\n",
    "#                    ndl.Tensor(np.random.randn(9,8)),\n",
    "#                    ndl.Tensor(np.random.randn(10,8)), backward=True)\n",
    "#\n",
    "#\n",
    "# gradient_check(lambda A,B : ndl.summation(ndl.broadcast_to(A,shape=(10,9))*B, axes=None),\n",
    "#                    ndl.Tensor(np.random.randn(10,1)),\n",
    "#                    ndl.Tensor(np.random.randn(10,9)), backward=True)\n",
    "# print(ndl.Tensor(np.array(1)).shape)\n",
    "# gradient_check(lambda A, : A.sum(axes=1).sum(),\n",
    "#                    ndl.Tensor(np.array([[1.0, 2.0], [3.0, 2.0], [2.3,3.4]])),\n",
    "#                      backward=True)\n",
    "t1 = ndl.Tensor([[1.0],[1.0],[1.0]])\n",
    "t3 = ndl.Tensor([[1.0,1.0], [1.0,1.0], [1.0,1.0]])\n",
    "t2 = ndl.Tensor([[2.0,1.0],[2.0,2.0],[2.0,2.0]])\n",
    "# # print(t1.shape, t2.shape)\n",
    "# # print(t1[1])\n",
    "# # t1 = ndl.Tensor(np.random.randn(3,1))\n",
    "# # t2 = ndl.Tensor(np.random.randn(3,2))\n",
    "gradient_check(lambda A,B : ndl.summation(ndl.broadcast_to(A,shape=(10,9))*B, axes=None),\n",
    "               ndl.Tensor(np.random.randn(10,1)),\n",
    "               ndl.Tensor(np.random.randn(10,9)), backward=True)\n",
    "# gradient_check(lambda A: A.sum(), t3, backward=True)\n",
    "# gradient_check(lambda A,B,C : ndl.summation(ndl.reshape(A,shape=(10,10))@B/5+C, axes=None),\n",
    "#                    ndl.Tensor(np.random.randn(100)),\n",
    "#                    ndl.Tensor(np.random.randn(10,5)),\n",
    "#                    ndl.Tensor(np.random.randn(10,5)), backward=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%a = ndl.tensor(\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from apps.simple_ml import *\n",
    "import numdifftools as nd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%def test_softmax_loss_ndl():\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,-1],[0,1]]).astype(np.float32)\n",
    "t = ndl.Tensor(a)\n",
    "print(np.mean(a))\n",
    "# gradient_check(lambda A : ndl.relu(A) , t, tol=0.01,  backward=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def ssoftmax_loss(Z, y):\n",
    "    \"\"\" Return softmax loss.  Note that for the purposes of this assignment,\n",
    "    you don't need to worry about \"nicely\" scaling the numerical properties\n",
    "    of the log-sum-exp computation, but can just compute this directly.\n",
    "\n",
    "    Args:\n",
    "        Z (np.ndarray[np.float32]): 2D numpy array of shape\n",
    "            (batch_size, num_classes), containing the logit predictions for\n",
    "            each class.\n",
    "        y (np.ndarray[np.int8]): 1D numpy array of shape (batch_size, )\n",
    "            containing the true label of each example.\n",
    "\n",
    "    Returns:\n",
    "        Average softmax loss over the sample.\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "    t = np.log((np.e ** Z).sum(axis=1)) - Z[np.arange(0, Z.shape[0]), y]\n",
    "    # print(\"standard t: \", t)\n",
    "    return np.mean(np.log((np.e ** Z).sum(axis=1)) - Z[np.arange(0, Z.shape[0]), y])\n",
    "    ### END YOUR CODE\n",
    "def nnn_epoch(X, y, W1, W2, lr = 0.1, batch=100):\n",
    "    \"\"\" Run a single epoch of SGD for a two-layer neural network defined by the\n",
    "    weights W1 and W2 (with no bias terms):\n",
    "        logits = ReLU(X * W1) * W2\n",
    "    The function should use the step size lr, and the specified batch size (and\n",
    "    again, without randomizing the order of X).  It should modify the\n",
    "    W1 and W2 matrices in place.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray[np.float32]): 2D input array of size\n",
    "            (num_examples x input_dim).\n",
    "        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)\n",
    "        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape\n",
    "            (input_dim, hidden_dim)\n",
    "        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape\n",
    "            (hidden_dim, num_classes)\n",
    "        lr (float): step size (learning rate) for SGD\n",
    "        batch (int): size of SGD minibatch\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ### BEGIN YOUR CODE\n",
    "\n",
    "    def relu(x):\n",
    "        x[x < 0] = 0\n",
    "        return x\n",
    "\n",
    "    def normalize(x):\n",
    "        return x / x.sum(axis=1, keepdims=1)\n",
    "    # W1 = W1.numpy()\n",
    "    # W2 = W2.numpy()\n",
    "\n",
    "    index = np.arange(0, X.shape[0], batch)\n",
    "    if index[-1] < X.shape[0]:\n",
    "        index = np.append(index, X.shape[0])\n",
    "    for i in range(len(index) - 1):\n",
    "        x_batch = X[index[i] : index[i + 1]]\n",
    "        y_batch = y[index[i] : index[i + 1]]\n",
    "        # print(\"reference x_batch\", x_batch)\n",
    "        # print(\"reference x * w1: \", x_batch @ W1)\n",
    "        Z1 = relu(x_batch @ W1)\n",
    "        # Z1 = x_batch @ W1\n",
    "        # print(\"reference relu: \", Z1)\n",
    "        I_y = np.zeros((batch, W2.shape[1]))\n",
    "        I_y[np.arange(0, batch), y_batch] = 1\n",
    "        G2 = normalize(np.e ** (Z1 @ W2)) - I_y\n",
    "        t = np.zeros_like(Z1)\n",
    "        t[Z1 > 0] = 1\n",
    "        G1 = t * (G2 @ W2.transpose((1, 0)))\n",
    "        delta_w1 = x_batch.transpose((1, 0)) @ G1 / batch\n",
    "        delta_w2 = Z1.transpose((1, 0)) @ G2 / batch\n",
    "        print(\"s_loss:\", ssoftmax_loss(Z1 @ W2, y_batch))\n",
    "        W1 -= lr * delta_w1\n",
    "        W2 -= lr * delta_w2\n",
    "        # print(delta_w1, delta_w2)\n",
    "    return W1, W2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(ndl.relu(ndl.Tensor([[-46.9 , -48.8 , -45.45, -49.  ],\n",
    "       [-49.75, -48.75, -45.8 , -49.25],\n",
    "       [-45.65, -45.25, -49.3 , -47.65]])).numpy(), np.array([[0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0.]]))\n",
    "gradient_check(ndl.relu, ndl.Tensor(np.random.randn(5,4)))\n",
    "\n",
    "# test nn gradients\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(50,5).astype(np.float64)\n",
    "y = np.random.randint(3, size=(50,)).astype(np.uint8)\n",
    "W1 = np.random.randn(5, 10).astype(np.float64) / np.sqrt(10)\n",
    "W2 = np.random.randn(10, 3).astype(np.float64) / np.sqrt(3)\n",
    "W1_0, W2_0 = W1.copy(), W2.copy()\n",
    "W1 = ndl.Tensor(W1)\n",
    "W2 = ndl.Tensor(W2)\n",
    "X_ = ndl.Tensor(X)\n",
    "y_one_hot = np.zeros((y.shape[0], 3))\n",
    "y_one_hot[np.arange(y.size), y] = 1\n",
    "y_ = ndl.Tensor(y_one_hot)\n",
    "dW1 = nd.Gradient(lambda W1_ :\n",
    "    softmax_loss(ndl.relu(X_@ndl.Tensor(W1_).reshape((5,10)))@W2, y_).numpy())(W1.numpy())\n",
    "dW2 = nd.Gradient(lambda W2_ :\n",
    "    softmax_loss(ndl.relu(X_@W1)@ndl.Tensor(W2_).reshape((10,3)), y_).numpy())(W2.numpy())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.437786\n"
     ]
    }
   ],
   "source": [
    "# W1, W2 = nn_epoch(X, y, W1, W2, lr=1.0, batch=50)\n",
    "# print(\"dw1\", dW1.reshape(5, 10))\n",
    "# print(\"dw2\", dW2.reshape(10, 3))\n",
    "# np.testing.assert_allclose(dW1.reshape(5,10), W1_0-W1.numpy(), rtol=1e-4, atol=1e-4)\n",
    "# np.testing.assert_allclose(dW2.reshape(10,3), W2_0-W2.numpy(), rtol=1e-4, atol=1e-4)\n",
    "\n",
    "# test full epoch\n",
    "X,y = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "                  \"data/train-labels-idx1-ubyte.gz\")\n",
    "np.random.seed(0)\n",
    "W1 = ndl.Tensor(np.random.randn(X.shape[1], 100).astype(np.float32) / np.sqrt(100))\n",
    "W2 = ndl.Tensor(np.random.randn(100, 10).astype(np.float32) / np.sqrt(10))\n",
    "# W11 = np.random.randn(X.shape[1], 100).astype(np.float64) / np.sqrt(100)\n",
    "# W22 = np.random.randn(100, 10).astype(np.float64) / np.sqrt(10)\n",
    "W1, W2 = nn_epoch(X, y, W1, W2, lr=0.2, batch=100)\n",
    "# W11, W22 = nnn_epoch(X, y, W11, W22, 0.2, 100)\n",
    "# print(np.linalg.norm(W11))\n",
    "print(np.linalg.norm(W1.numpy()))\n",
    "np.testing.assert_allclose(np.linalg.norm(W1.numpy()), 28.437788,\n",
    "                           rtol=1e-5, atol=1e-5)\n",
    "np.testing.assert_allclose(np.linalg.norm(W2.numpy()), 10.455095,\n",
    "                           rtol=1e-5, atol=1e-5)\n",
    "np.testing.assert_allclose(loss_err(ndl.relu(ndl.Tensor(X)@W1)@W2, y),\n",
    "                           (0.19770025, 0.06006667), rtol=1e-4, atol=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-10-7a3c6c1ac70e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[0mz\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0mndl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m9\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;31m# gradient_check(lambda Z,y_one_hot : (ndl.log((Z).sum(axes=1))).sum() , z, y_one_hot, tol=0.01,  backward=True)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m \u001B[0mnnn_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw11\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mw22\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;31m# test full epoch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;31m# X,y = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-7-5cb1f948d950>\u001B[0m in \u001B[0;36mnnn_epoch\u001B[1;34m(X, y, W1, W2, lr, batch)\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[1;31m# print(\"reference x_batch\", x_batch)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     61\u001B[0m         \u001B[1;31m# print(\"reference x * w1: \", x_batch @ W1)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 62\u001B[1;33m         \u001B[0mZ1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_batch\u001B[0m \u001B[1;33m@\u001B[0m \u001B[0mW1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     63\u001B[0m         \u001B[1;31m# Z1 = x_batch @ W1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m         \u001B[1;31m# print(\"reference relu: \", Z1)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# nnn_epoch(X, y, W1, W2, lr=1.0, batch=50)\n",
    "# nn_epoch(X, y, ndl.Tensor(W1_0), ndl.Tensor(W2_0), lr=0.1, batch=50)\n",
    "# np.testing.assert_allclose(dW1.reshape(5,10), W1_0-W1.numpy(), rtol=1e-4, atol=1e-4)\n",
    "# np.testing.assert_allclose(dW2.reshape(10,3), W2_0-W2.numpy(), rtol=1e-4, atol=1e-4)\n",
    "x = np.array([[1,2,3], [4,5,6]]).astype(np.float64)\n",
    "y_one_hot = ndl.Tensor(np.array([[1,0], [0,1]]))\n",
    "y = np.array([0, 1])\n",
    "w1 = ndl.Tensor(np.array([[1,2], [1,3], [4,0]]).astype(np.float64))\n",
    "w2 = ndl.Tensor(np.array([[1,0], [1,2]]).astype(np.float64))\n",
    "nn_epoch(x, y, w1, w2, lr=0.1, batch=2)\n",
    "# Z = ndl.Tensor(np.array([[3, 0], [9, 0]]).astype(np.float64))\n",
    "# t = ndl.log((ndl.exp(Z)).sum(axes=1)) - (Z * y_one_hot).sum(axes=1)\n",
    "# tt = Z.sum(axes=1)\n",
    "# t = ndl.log(tt)\n",
    "# ans = t.sum()\n",
    "# Z = ndl.Tensor(x) @ w1 @ w2\n",
    "# t1 = ndl.log((ndl.exp(Z)).sum(axes=1)) - (Z * y_one_hot).sum(axes=1)\n",
    "# ans = t1.sum() / t1.shape[0]\n",
    "# ans.backward()\n",
    "# print(ans)\n",
    "# print(\"my grad\", w1.grad,w2.grad)\n",
    "w11 = ndl.Tensor(np.array([[1,2], [1,3], [4,0]]).astype(np.float64))\n",
    "w22 = ndl.Tensor(np.array([[1,0], [1,2]]).astype(np.float64))\n",
    "z =ndl.Tensor(np.array([[3, 0], [9, 0]]).astype(np.float64))\n",
    "# gradient_check(lambda Z,y_one_hot : (ndl.log((Z).sum(axes=1))).sum() , z, y_one_hot, tol=0.01,  backward=True)\n",
    "nnn_epoch(x, y, w11, w22, lr=0.1, batch=2)\n",
    "# test full epoch\n",
    "# X,y = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
    "#                   \"data/train-labels-idx1-ubyte.gz\")\n",
    "# np.random.seed(0)\n",
    "# W1 = ndl.Tensor(np.random.randn(X.shape[1], 100).astype(np.float32) / np.sqrt(100))\n",
    "# W2 = ndl.Tensor(np.random.randn(100, 10).astype(np.float32) / np.sqrt(10))\n",
    "# W1, W2 = nn_epoch(X, y, W1, W2, lr=0.2, batch=100)\n",
    "# np.testing.assert_allclose(np.linalg.norm(W1.numpy()), 28.437788,\n",
    "#                            rtol=1e-5, atol=1e-5)\n",
    "# np.testing.assert_allclose(np.linalg.norm(W2.numpy()), 10.455095,\n",
    "#                            rtol=1e-5, atol=1e-5)\n",
    "# np.testing.assert_allclose(loss_err(ndl.relu(ndl.Tensor(X)@W1)@W2, y),\n",
    "#                            (0.19770025, 0.06006667), rtol=1e-4, atol=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "a = np.broadcast_to(a ,(2,2))\n",
    "print(a)\n",
    "\n",
    "x = ndl.Tensor(np.array([[1,2,3], [4,5,6]]))\n",
    "y_one_hot = ndl.Tensor(np.array([[1,0], [0,1]]))\n",
    "y = ndl.Tensor(np.array([0, 1]))\n",
    "w1 = ndl.Tensor(np.array([[0,0], [0,0], [0,0]]))\n",
    "w2 = ndl.Tensor(np.array([[0,0], [0,0]]))\n",
    "nn_epoch(x, y, w1, w2, lr=0.1, batch=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
